# -*- coding: utf-8 -*-
"""spec_class

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hWZhfuzlONNHGvbvERQfdjEyUBfUbH_C
"""

!pip install librosa tensorflow scikit-learn tqdm matplotlib

import os, random, glob
import librosa
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from tqdm import tqdm
import matplotlib


# For Google Colab with Google Drive:
#from google.colab import drive
#drive.mount('/content/drive')
#DATASET_PATH = "/content/drive/MyDrive/seabird3s" # Adjust path as needed
# For local execution (uncomment and adjust path):
# DATASET_PATH = "/content/Downloads/seabird3s" # Local path to your dataset

# Alternative: Upload dataset directly to Colab (temporary)
# from google.colab import files
# uploaded = files.upload() # Then extract if needed
# Assuming the zip file was uploaded and extracted to /content/seabird3s
DATASET_PATH = "/content/seabird3s" # Update this path to your extracted dataset folder

import zipfile
import os

# Assuming your zip file is in the /content/ directory after uploading
zip_file_path = "/content/Downloads/seabird3s.zip" # Replace with the actual name of your zip file
extract_path = "/content"

if os.path.exists(zip_file_path):
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    print(f"Extracted {zip_file_path} to {extract_path}")

    # After extraction, you'll need to find the name of the extracted folder.
    # It's often the same as the zip file name without the .zip extension,
    # or it might be a different name depending on how the zip was created.
    # You can list the contents of /content/ to find the correct folder name.
    print("Contents of /content/ after extraction:")
    print(os.listdir("/content/Downloads"))

else:
    print(f"Error: Zip file not found at {zip_file_path}")
    print("Please make sure you have uploaded the zip file and the path is correct.")

# Check if dataset path exists
if os.path.exists(DATASET_PATH):
    print(f"Dataset path found: {DATASET_PATH}")
    # List classes (subfolders)
    class_names = [d for d in os.listdir(DATASET_PATH)
                   if os.path.isdir(os.path.join(DATASET_PATH, d)) and not d.startswith('.')]
    print(f"Found {len(class_names)} classes: {class_names}")
    # Count files in each class
    for class_name in class_names:
        class_path = os.path.join(DATASET_PATH, class_name)
        wav_files = [f for f in os.listdir(class_path) if f.endswith('.wav')]
        print(f"  {class_name}: {len(wav_files)} WAV files")
else:
    print(f"Error: Dataset path not found: {DATASET_PATH}")
    print("Please check the path and try again.")

# =====================
# Step 1: Collect file paths & split train/val/test
# =====================
file_paths = glob.glob(os.path.join(DATASET_PATH, "*/*.wav"))
random.shuffle(file_paths)

# Extract class names from folder structure
class_names = sorted([d for d in os.listdir(DATASET_PATH) if os.path.isdir(os.path.join(DATASET_PATH, d))])
class_to_index = {name: idx for idx, name in enumerate(class_names)}

# Create labels
labels = [class_to_index[os.path.basename(os.path.dirname(f))] for f in file_paths]

# Manual split (80% train, 10% val, 10% test)
train_split = int(0.8 * len(file_paths))
val_split   = int(0.9 * len(file_paths))

train_files, val_files, test_files = file_paths[:train_split], file_paths[train_split:val_split], file_paths[val_split:]
train_labels, val_labels, test_labels = labels[:train_split], labels[train_split:val_split], labels[val_split:]

print(f"Classes: {class_names}")
print(f"Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}")

# =====================
# Step 2: Audio → Mel spectrogram function
# =====================
def wav_to_mel(path, label, sr=44100, n_mels=224, duration=3.0, img_size=(224,224)):
    audio, sr = librosa.load(path.numpy().decode(), sr=sr, duration=duration)

    # Mel spectrogram
    S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels, n_fft=2048, hop_length=512)
    S_db = librosa.power_to_db(S, ref=np.max)

    # Normalize 0–1
    S_norm = (S_db - S_db.min()) / (S_db.max() - S_db.min() + 1e-8)

    # Convert to RGB using viridis colormap
    cmap = plt.get_cmap("viridis")
    S_rgb = cmap(S_norm)[..., :3]

    # Resize to 224×224
    S_resized = tf.image.resize(S_rgb, img_size).numpy().astype(np.float32)

    return S_resized, label

def tf_wav_to_mel(path, label):
    spectrogram, label = tf.py_function(wav_to_mel, [path, label], [tf.float32, tf.int32])
    spectrogram.set_shape((224, 224, 3))
    label.set_shape(())
    return spectrogram, label

# =====================
# Step 3: Build tf.data pipelines
# =====================
from keras.applications import MobileNetV3Small
from keras.applications.mobilenet_v3 import preprocess_input

batch_size = 32

def build_dataset(file_list, label_list, cache_name):
    ds = tf.data.Dataset.from_tensor_slices((file_list, label_list))
    ds = ds.map(tf_wav_to_mel, num_parallel_calls=tf.data.AUTOTUNE)

    # Apply MobileNetV3 preprocessing
    ds = ds.map(lambda x, y: (preprocess_input(x * 255.0), y),
                num_parallel_calls=tf.data.AUTOTUNE)

    # Cache to local disk
    ds = ds.cache(cache_name).batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return ds

train_ds = build_dataset(train_files, train_labels, "/content/train_cache")
val_ds   = build_dataset(val_files, val_labels, "/content/val_cache")
test_ds  = build_dataset(test_files, test_labels, "/content/test_cache")

# =====================
# Step 4: Build & compile model
# =====================
base_model = MobileNetV3Small(
    input_shape=(224,224,3),
    include_top=False,
    weights="imagenet"
)
base_model.trainable = False

model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(len(class_names), activation="softmax")
])

model.compile(optimizer=tf.keras.optimizers.Adam(0.001),
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

model.summary()

# =====================
# Step 5: Train
# =====================
history = model.fit(train_ds, validation_data=val_ds, epochs=20)

from sklearn.metrics import classification_report
 print("Evaluating model on test set...")
 # Get test accuracy
 test_loss, test_accuracy = model.evaluate(test_ds, verbose=0)
 print(f"Test Accuracy: {test_accuracy:.1%}")
 # Get predictions
 predictions = model.predict(test_ds, verbose=0)
 y_pred = np.argmax(predictions, axis=1)
 # Generate comprehensive classification report

 # Collect predictions for sklearn report
 X_test, y_test = [], []
 for x_batch, y_batch in test_ds:
    X_test.append(x_batch.numpy())
    y_test.append(y_batch.numpy())
 X_test = np.concatenate(X_test, axis=0)
 y_test = np.concatenate(y_test, axis=0)

 preds = model.predict(test_ds, verbose=0)
 y_pred = np.argmax(preds, axis=1)

 print("\n=== CLASSIFICATION REPORT ===")
 print(classification_report(y_test, y_pred, target_names=class_names, digits=3)
 )
 # Display dataset statistics for context
 print(f"\nDataset Statistics:")
 print(f"-Total samples: {len(file_paths)}")
 print(f"-Number of classes: {len(class_names)}")
 print(f"-Test set size: {len(X_test)} samples ({len(X_test)/len(file_paths)*100:.1f} %of total)")

# Create confusion matrix
 cm = confusion_matrix(y_test, y_pred)
 # Plot confusion matrix
 plt.figure(figsize=(10, 8))
 disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
 disp.plot(cmap='Blues', xticks_rotation=45)
 plt.title(f'Confusion Matrix- Test Accuracy: {test_accuracy:.1%}')
 plt.tight_layout()
 plt.show()

print("\n=== SAMPLE PREDICTIONS ===")
for i in range(min(5, len(X_test))): # Show up to 5 examples
    true_class = class_names[y_test[i]]
    pred_class = class_names[y_pred[i]]
    confidence = np.max(predictions[i])
    status = "Correct" if y_test[i] == y_pred[i] else "Wrong"
    print(f"{status} - True: {true_class}, Predicted: {pred_class} (confidence: {confidence:.2f})")

def plot_training_history(history):
    plt.figure(figsize=(12, 4))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train (Phase 1)')
    plt.plot(history.history['val_accuracy'], label='Val (Phase 1)')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train (Phase 1)')
    plt.plot(history.history['val_loss'], label='Val (Phase 1)')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

plot_training_history(history)
